
**使用Kubeadm搭建Kubernetes1.19.0集群，宿主机使用9.1里搭建的master、node1、node2、node3四台虚拟机，即一主三从的K8S集群结构**

# 目录

* [1. 安装准备](#1-安装准备)
  * [master虚拟机安装准备](#master虚拟机安装准备)
  * [node1虚拟机安装准备](#node1虚拟机安装准备)
  * [node2虚拟机安装准备](#node2虚拟机安装准备)
  * [node3虚拟机安装准备](#node3虚拟机安装准备)
* [2. 安装kubeadm等工具](#2-安装kubeadm等工具)  
  * [master机上安装kubeadm等工具](#master机上安装kubeadm等工具)
  * [node1机上安装kubeadm等工具](#node1机上安装kubeadm等工具)
  * [node2机上安装kubeadm等工具](#node2机上安装kubeadm等工具)
  * [node3机上安装kubeadm等工具](#node3机上安装kubeadm等工具)
* [3. 安装Master节点](#3-安装Master节点)  
  * [3.1 安装Master节点在master虚拟机上](#安装Master节点在master虚拟机上)
* [4. 安装Node节点](#4-安装Node节点)
  * [4.1 在node1虚拟机上安装Node节点](#在node1虚拟机上安装Node节点)
  * [4.2 在node2虚拟机上安装Node节点](#在node2虚拟机上安装Node节点)
  * [4.3 在node3虚拟机上安装Node节点](#在node3虚拟机上安装Node节点)
---

# 1 安装准备

**在搭建K8S集群之前，我们需要对虚拟机进行一些操作。下面这些操作需要在master、node1、node2、node3四台虚拟机上执行，这里以master为例,但是我会把每一台机的配置步骤都写出来**

## master虚拟机安装准备

    1.安装必要软件：

        [root@master]# yum install -y net-tools.x86_64 wget
    
    2.配置hosts：

        [root@master]# vi /etc/hosts
        内容如下所示：

        192.168.33.11 master
        192.168.33.12 node1
        192.168.33.13 node2
        192.168.33.14 node3

    3.关闭防火墙：

      为了避免kubernetes的Master节点和各个工作节点的Node节点间的通信出现问题，我们可以关闭本地搭建的Centos虚拟机的防火墙。

      [root@master]# systemctl disable firewalld
      [root@master]# systemctl stop firewalld

    4.禁用SELinux，让容器可以顺利地读取主机文件系统：
      
      //临时关闭SELinux时
      [root@master]# setenforce 0

      //SELINUX=enforcing则表示强制启动SELinux，SELINUX=permissive 表示临时不强制起作用，但会生成警告信息；SELINUX=disabled 完全关闭SELinux
      [root@master]# sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config
      
     
    5.修改Docker配置：

        [root@master]# vi /etc/docker/daemon.json
        在{}内追加如下内容：

        "exec-opts": ["native.cgroupdriver=systemd"]

        内容如下：
        
        {
          “registry-mirrors”：[                             //配置镜像加速器
             "https://dockerhub.azk8s.cn",
             "https://reg-mirror.qiniu.com",
             "https://registry.docker-cn.com"
          ],
          "exec-opts": ["native.cgroupdriver=systemd"]     //新增加的内容  修改cgroupdriver是为了消除告警：[WARNING IsDockerSystemdCheck]: detected “cgroupfs” as the Docker 
                                                                          cgroup driver. The recommended driver is “systemd”. Please follow the guide 
                                                                          at https://kubernetes.io/docs/setup/cri/
        }

        重启docker：

        [root@master]# systemctl daemon-reload
        [root@master]# systemctl restart docker

      6. 内核参数修改, 将桥接的IPv4流量传递到iptables的链,
         本文的k8s网络使用flannel，该网络需要设置内核参数bridge-nf-call-iptables=1，修改这个参数需要系统有br_netfilter模块
         
         6.1 br_netfilter模块加载
             查看br_netfilter模块：

             [root@master01 ~]# lsmod |grep br_netfilter
             如果系统没有br_netfilter模块则执行下面的新增命令，如有则忽略。

             临时新增br_netfilter模块:

             [root@master01 ~]# modprobe br_netfilter
             该方式重启后会失效

             or
             
             永久新增br_netfilter模块：

             [root@master01 ~]# cat > /etc/rc.sysinit << EOF
             #!/bin/bash
             for file in /etc/sysconfig/modules/*.modules ; do
             [ -x $file ] && $file
             done
             EOF
             
             [root@master01 ~]# cat > /etc/sysconfig/modules/br_netfilter.modules << EOF
             modprobe br_netfilter
             EOF
                          
             [root@master01 ~]# chmod 755 /etc/sysconfig/modules/br_netfilter.modules

         6.2 内核参数修改
       
            // 配置 iptables桥接及路由转发
            //  内核参数临时修改
            
            [root@master]# sysctl net.bridge.bridge-nf-call-iptables = 1
            [root@master]# sysctl net.bridge.bridge-nf-call-ip6tables = 1
            
            or 
            
            //内核参数永久修改
        
             [root@master]#cat > /etc/sysctl.d/k8s.conf << EOF
                           >  net.bridge.bridge-nf-call-ip6tables = 1
                           >  net.bridge.bridge-nf-call-iptables = 1
                           //可以选择性再增加如下参数，但是如上参数是必需的
                           
                           > net.ipv4.ip_forward = 1
                           > net.ipv4.ip_nonlocal_bind = 1
                           > net.unix.max_dgram_qlen=128
                           > net.ipv4.conf.all.rp_filter=0
                           > net.ipv4.conf.default.rp_filter=0
                           
                           >  EOF

             [root@master]# sysctl --system
         
             or 
         
             //  从指定的配置文件 K8s.conf 中加载刚修改的配置参数
             [root@master]# sysctl -p /etc/sysctl.d/k8s.conf
             
             or 
             
             //  从指定的配置文件 /etc/sysctl.conf 中加载刚修改的配置参数,刚刚修改的配置参数必需是在/etc/sysctl.conf文件上
             [root@master]# sysctl -p

      7.关闭swap

          Swap是操作系统在内存吃紧的情况申请的虚拟内存，按照Kubernetes官网的说法，Swap会对Kubernetes的性能造成影响，不推荐使用Swap。

          [root@master]# echo "vm.swappiness = 0">> /etc/sysctl.conf 
          [root@master]# swapoff -a        //临时禁用
          
          or
          
          //若需要重启后也生效，在禁用swap后还需修改配置文件/etc/fstab，注释swap
          [root@master]# sed -i.bak '/swap/s/^/#/' /etc/fstab       //永久禁用

## node1虚拟机安装准备


## node2虚拟机安装准备

## node3虚拟机安装准备

---

# 2 安装kubeadm等工具

**准备工作完毕后，接着在master、node1、node2、node3四台虚拟机上安装kubeadm相关工具。下面这些操作需要在master、node1、node2、node3四台虚拟机上执行，这里以master为例**

## master机上安装kubeadm等工具

         1.配置kubernetes源：

          [root@master]# vi /etc/yum.repos.d/kubernetes.repo

                         [kubernetes]
                         name=Kubernetes
                         baseurl= https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/   //配置国内的kubernetes源 
                         or 
                         
                         baseurl= https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/                  // 配置国外的kubernetes
                         enabled=1
                         gpgcheck=1
                         repo_gpgcheck=1
                         gpgkey= https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg    //国内
                         
                         or 
                         
                         gpgkey= https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg   //国外
                         
                         exclude=kubelet kubeadm kubectl
                         
 ---
                         
                         
                       [] 中括号中的是repository id，唯一，用来标识不同仓库
                       name 仓库名称，自定义
                       baseurl 仓库地址
                       enable 是否启用该仓库，默认为1表示启用
                       gpgcheck 是否验证从该仓库获得程序包的合法性，1为验证
                       repo_gpgcheck 是否验证元数据的合法性 元数据就是程序包列表，1为验证
                       gpgkey=URL 数字签名的公钥文件所在位置，如果gpgcheck值为1，此处就需要指定gpgkey文件的位置，如果gpgcheck值为0就不需要此项了  
         
         2. 更新缓存
            [root@master]# yum clean all
            [root@master]# yum -y makecache
          
         3.安装kubelet、kubeadm和kubectl工具：

           //版本查看
           [root@master]#   yum list kubelet --showduplicates | sort -r 

           //若不指定版本直接运行‘yum install -y kubelet kubeadm kubectl’则默认安装最新版
           // 现在是指定kubernetes 的版本
           
           [root@master]# yum install -y kubelet-1.19.3-0 kubeadm-1.19.3-0 kubectl-1.19.3-0 --disableexcludes=kubernetes

         4.启动kubelet并设置开机自启：     
         
           [root@master]# systemctl enable kubelet
           [root@master]# systemctl start kubelet

           提示：此时kubelet的服务运行状态是异常的，因为缺少主配置文件kubelet.conf。但可以暂不处理，因为在完成Master节点的初始化后才会生成这个配置文件。



## node1机上安装kubeadm等工具

## node2机上安装kubeadm等工具

## node3机上安装kubeadm等工具

---

# 3 安装Master节点

## 安装Master节点在master虚拟机上

**安装好kubeadm等相关工具后，接着在master虚拟机上执行以下操作：使用下面这条命令启动 master节点**

    3.1 初始化Master
        
        //使用 kubeadm 初始化集群（只在master节点上执行）
        [root@master]# kubeadm init //--kubernetes-version=v1.19.3    
                                    --pod-network-cidr=10.244.0.0/16 
                                    --service-cidr=10.1.0.0/16 
                                    --apiserver-advertise-address=192.168.33.11 
                                    //--image-repository registry.aliyuncs.com/google_containers
      
      配置含义如下：

       kubernetes-version: 用于指定k8s版本，这里指定为最新的1.19.3版本；
       apiserver-advertise-address：用于指定kube-apiserver监听的ip地址，就是master本机IP地址。
       pod-network-cidr：因为后面我们选择flannel作为Pod的网络插件，所以这里需要指定Pod的网络范围为10.244.0.0/16
       service-cidr：用于指定SVC的网络范围；
       image-repository: 其中默认的镜像仓库k8s.gcr.io没有科学上网的话无法访问，我们可以将它修改为国内的阿里镜像仓
                         库registry.aliyuncs.com/google_containers

       启动成功后，你会看到类似如下提示:
       
            Your Kubernetes control-plane has initialized successfully!

            To start using your cluster, you need to run the following as a regular user:

              mkdir -p $HOME/.kube
              sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
              sudo chown $(id -u):$(id -g) $HOME/.kube/config

            You should now deploy a pod network to the cluster.
            Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
              https://kubernetes.io/docs/concepts/cluster-administration/addons/

            Then you can join any number of worker nodes by running the following on each as root:

            //而下面这段则是用于工作节点Node加入Master集群用的，而它的令牌号数是每次执行 kubernetes init 命令都不同的
            
            kubeadm join 192.168.33.11:6443 --token 0na0jo.e9svsj5tmeciad88 --discovery-token-ca-cert-hash sha256:5fbcc1a1167d0
                                      db98d0fc6c037a06378899978d4c811c61a271f2bf116810e93
            
            
  <a href="https://ibb.co/wMsPnZ7"><img src="https://i.ibb.co/6gBTKc8/kubeadm-token-number.jpg" alt="kubeadm-token-number" border="0"></a>
            
          等待一段时间，看到successful即说明Kubernetes集群初始化完成。加载kubeadm认证到系统环境变量中:
            
          //加载kubeadm认证到系统环境变量           
    
         //如果是root用户
         [root@master ~]# echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /vagrant/.bash_profile
         [root@master ~]# cd /vagrant
         [root@master vagrant]# source .bash_profile 

         or 
         
         //若为非root用户
         [root@master]# sudo mkdir -p $HOME/.kube
         [root@master]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
         [root@master]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
       
       初始化失败：

         如果初始化失败，可执行kubeadm reset后重新初始化
         [root@master01 ~]# kubeadm reset
         [root@master01 ~]# rm -rf $HOME/.kube/config
       
       
    3.2 安装pod网络
        在master上执行kubectl get nodes命令，会发现Kubernetes提示Master为NotReady状态，这是因为还没有安装网络插件
        [root@master]# kubectl get nodes
        
  <a href="https://ibb.co/Cs4DzRh"><img src="https://i.ibb.co/BKvFt0q/flannel2.png" alt="flannel2" border="0"></a>
        
        //对于网络插件，可以有许多选择，请参考https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network的说明。这里我选择的flannel
        
        [root@master]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  //一般会出问题 可先下载，
      
        ////在master虚拟机上下载flannel配置文件：
        [root@master]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 
       
        //修改kube-flannel.yml：
        [root@master]# vi kube-flannel.yml
        
        containers:
             - name: kube-flannel
               image: quay.io/coreos/flannel:v0.11.0-amd64
               command:
               - /opt/bin/flanneld
               args:
               - --ip-masq
               - --kube-subnet-mgr
               - --iface=eth1 # 新增部分
        
         //Vagrant 在多主机模式下有多个网卡，eth0 网卡用于nat转发访问公网，而eth1网卡才是主机真正的IP，在这种情况下直接部署k8s flannel 插件会导致CoreDNS无法工作，所以我们需要添加上面这条
           配置强制flannel使用eth1
           
        //安装flannel：

        [root@master]# kubectl create -f kube-flannel.yml  
        
        输出如下所示时，表示安装成功
        
  <a href="https://ibb.co/WgXpnw7"><img src="https://i.ibb.co/f4WGM5Z/flannel.png" alt="flannel" border="0"></a>
               
        再次查看节点状态：

        [root@master]# kubectl get nodes       
        
  <a href="https://ibb.co/BG3Fd6g"><img src="https://i.ibb.co/hc9whdK/flannel3.png" alt="flannel3" border="0"></a>
        
        
        可以看到所有节点都是Ready状态。
        
        执行kubectl get pods --all-namespaces，验证Kubernetes集群的相关Pod是否都正常创建并运行
        
  <a href="https://ibb.co/bR35rGH"><img src="https://i.ibb.co/5x9h5JY/flannel4.png" alt="flannel4" border="0"></a>        
  
  到这里通过Kubeadm安装Kubernetesv1.19.0 集群已经成功了
  
        
        
# 4 安装Node节点

**接着在node1、node2、node3三台虚拟机上安装Node节点，在node1、node2和node3节点上执行下面这条命令，加入Master**
**注意， 在Node 节点运行kubeadm join 命令时需要使用Root 用户（或使用sudo 执行）, 因为kube-proxy 服务需要操作系统的iptables 服务，必须获得管理员权限**

## 在node1虚拟机上安装Node节点
   
   [root@node1]# swapoff -a 
   [root@node1]# systemctl enable --now kubelet
   [root@node1]# systemctl start kubelet
   
   [root@node1]# kubeadm join 192.168.33.11:6443 --token 0na0jo.e9svsj5tmeciad88 --discovery-token-ca-cert-hash sha256:5fbcc1a1167d0
                                      db98d0fc6c037a06378899978d4c811c61a271f2bf116810e93

## 在node2虚拟机上安装Node节点

   [root@node1]# kubeadm join 192.168.33.11:6443 --token 0na0jo.e9svsj5tmeciad88 --discovery-token-ca-cert-hash sha256:5fbcc1a1167d0
                                      db98d0fc6c037a06378899978d4c811c61a271f2bf116810e93

## 在node3虚拟机上安装Node节点

   [root@node1]# kubeadm join 192.168.33.11:6443 --token 0na0jo.e9svsj5tmeciad88 --discovery-token-ca-cert-hash sha256:5fbcc1a1167d0
                                      db98d0fc6c037a06378899978d4c811c61a271f2bf116810e93
