
**使用Kubeadm搭建Kubernetes1.19.0集群，宿主机使用9.1里搭建的master、 node1、node2、node3四台虚拟机，可配一主三从的K8S集群结构，这以一主三从为例**

* [1. 一主三从架构](#一主三从架构)
* [2. 三主三从架构](https://github.com/stevenli91748/DEMO/blob/master/Spring%20Cloud%20%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9D%83%E9%99%90%E7%B3%BB%E7%BB%9F%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%93%8D---2020/%E7%AC%AC%E4%B9%9D%E7%AB%A0%20K8S%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/9.4%20%E7%94%A8Kubeadm%E6%90%AD%E5%BB%BA%E4%B8%9A%E5%8A%A1%E5%BE%AE%E6%9C%8D%E5%8A%A1K8S%E9%9B%86%E7%BE%A4---%20%E4%B8%89%E4%B8%BB%E4%B8%89%E4%BB%8E%E7%9A%84K8S%E9%9B%86%E7%BE%A4%E7%BB%93%E6%9E%84.md)


# 一主三从架构

* [1. 安装准备](#1-安装准备)
  * [master虚拟机安装准备](#master虚拟机安装准备)
  * [master2虚拟机安装准备](#master2虚拟机安装准备)
  * [master3虚拟机安装准备](#master3虚拟机安装准备)
  * [node1虚拟机安装准备](#node1虚拟机安装准备)
  * [node2虚拟机安装准备](#node2虚拟机安装准备)
  * [node3虚拟机安装准备](#node3虚拟机安装准备)
* [2. 安装kubeadm等工具](#2-安装kubeadm等工具)  
  * [master机上安装kubeadm等工具](#master机上安装kubeadm等工具)
  * [master2机上安装kubeadm等工具](#master2机上安装kubeadm等工具)
  * [master3机上安装kubeadm等工具](#master3机上安装kubeadm等工具)  
  * [node1机上安装kubeadm等工具](#node1机上安装kubeadm等工具)
  * [node2机上安装kubeadm等工具](#node2机上安装kubeadm等工具)
  * [node3机上安装kubeadm等工具](#node3机上安装kubeadm等工具)
* [3. 安装Master节点](#3-安装Master节点)  
  * [3.1 安装Master节点在master虚拟机上](#安装Master节点在master虚拟机上)
* [4. 安装Node节点](#4-安装Node节点)
  * [4.1 在node1虚拟机上安装Node节点](#在node1虚拟机上安装Node节点)
  * [4.2 在node2虚拟机上安装Node节点](#在node2虚拟机上安装Node节点)
  * [4.3 在node3虚拟机上安装Node节点](#在node3虚拟机上安装Node节点)
* [5. 配置网络](#5-配置网络)  
  * [5.1 配置 flannel 网络](#配置-flannel-网络) 
  * [5.2 配置 Calico 网络](#配置-Calico-网络)
* [6. 重启Kubernetes集群](#6-重启Kubernetes集群)  
---

# 1 安装准备

**在搭建K8S集群之前，我们需要对虚拟机进行一些操作。下面这些操作需要在master、node1、node2、node3四台虚拟机上执行，这里以master为例,但是我会把每一台机的配置步骤都写出来**

## master虚拟机安装准备

    1.安装必要软件：

        [root@master]# yum install -y net-tools.x86_64 wget
    
    2.配置hosts：

        [root@master]# vi /etc/hosts
        内容如下所示：

        192.168.33.11 master
        192.168.33.12 node1
        192.168.33.13 node2
        192.168.33.14 node3

       验证mac地址uuid
       
       mac地址
       [root@master]# cat /sys/class/net/ens/address
       00:0c:29:17:fd:e3
       
       uuid
       [root@master]# cat /sys/class/dmi/id/product_uuid
       oiuerj6oiej-3476-1234-467f-riuysjjnf657
       
 **保证各节点mac和uuid唯一**
       

    3.关闭防火墙：

      为了避免kubernetes的Master节点和各个工作节点的Node节点间的通信出现问题，我们可以关闭本地搭建的Centos虚拟机的防火墙。

      [root@master]# systemctl disable firewalld
      [root@master]# systemctl stop firewalld

    4.禁用SELinux，让容器可以顺利地读取主机文件系统：
    
      //关闭 SELinux，目的为了允许容器能够与本机文件系统交互
      //临时关闭SELinux时
      [root@master]# setenforce 0

      //SELINUX=enforcing则表示强制启动SELinux，SELINUX=permissive 表示临时不强制起作用，但会生成警告信息；SELINUX=disabled 完全关闭SELinux
      
      [root@master]# sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config
      
     
    5.修改Docker配置：

        [root@master]# vi /etc/docker/daemon.json
        在{}内追加如下内容：

        "exec-opts": ["native.cgroupdriver=systemd"]

        内容如下：
        
        {
          “registry-mirrors”：[                             //配置镜像加速器
             "https://dockerhub.azk8s.cn",
             "https://reg-mirror.qiniu.com",
             "https://registry.docker-cn.com"
          ],
          "exec-opts": ["native.cgroupdriver=systemd"]     //新增加的内容  修改cgroupdriver是为了消除告警：[WARNING IsDockerSystemdCheck]: detected “cgroupfs” as the Docker 
                                                                          cgroup driver. The recommended driver is “systemd”. Please follow the guide 
                                                                          at https://kubernetes.io/docs/setup/cri/
        }

        重启docker：

        [root@master]# systemctl daemon-reload
        [root@master]# systemctl restart docker

      6. 内核参数修改, 将桥接的IPv4流量传递到iptables的链,
         本文的k8s网络使用flannel，该网络需要设置内核参数bridge-nf-call-iptables=1，修改这个参数需要系统有br_netfilter模块
         
         6.1 br_netfilter模块加载
             查看br_netfilter模块：

             [root@master01 ~]# lsmod |grep br_netfilter
             如果系统没有br_netfilter模块则执行下面的新增命令，如有则忽略。

             临时新增br_netfilter模块:

             [root@master01 ~]# modprobe br_netfilter
             该方式重启后会失效

             or
             
             永久新增br_netfilter模块：

             [root@master01 ~]# cat > /etc/rc.sysinit << EOF
             #!/bin/bash
             for file in /etc/sysconfig/modules/*.modules ; do
             [ -x $file ] && $file
             done
             EOF
             
             [root@master01 ~]# cat > /etc/sysconfig/modules/br_netfilter.modules << EOF
             modprobe br_netfilter
             EOF
                          
             [root@master01 ~]# chmod 755 /etc/sysconfig/modules/br_netfilter.modules

         6.2 内核参数修改
       
            // 配置 iptables桥接及路由转发
            //  内核参数临时修改
            // 修改网络开启桥接网络支持
            // 本文的k8s网络使用flannel，该网络需要设置内核参数bridge-nf-call-iptables=1
            
            //安装 ebtables ethtool，否则后边执行 kubeadm init 的时候会报错                          
            //[root@k8s-master ~]# yum install ebtables ethtool -y
            //[root@k8s-master ~]# echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables
            
            
            [root@master]# sysctl net.bridge.bridge-nf-call-iptables = 1
            [root@master]# sysctl net.bridge.bridge-nf-call-ip6tables = 1
            
            or 
            
            //内核参数永久修改
        
        
             [root@master]#cat > /etc/sysctl.d/k8s.conf << EOF
                           >  net.bridge.bridge-nf-call-ip6tables = 1
                           >  net.bridge.bridge-nf-call-iptables = 1
                           //可以选择性再增加如下参数，但是如上参数是必需的
                           
                           > net.ipv4.ip_forward = 1
                           > net.ipv4.ip_nonlocal_bind = 1
                           > net.unix.max_dgram_qlen=128
                           > net.ipv4.conf.all.rp_filter=0
                           > net.ipv4.conf.default.rp_filter=0
                           
                           >  EOF

             [root@master]# sysctl --system
         
             or 
         
             //  从指定的配置文件 K8s.conf 中加载刚修改的配置参数
             [root@master]# sysctl -p /etc/sysctl.d/k8s.conf
             
             or 
             
             //  从指定的配置文件 /etc/sysctl.conf 中加载刚修改的配置参数,刚刚修改的配置参数必需是在/etc/sysctl.conf文件上
             [root@master]# sysctl -p

      7.关闭swap

          Swap是操作系统在内存吃紧的情况申请的虚拟内存，按照Kubernetes官网的说法，Swap会对Kubernetes的性能造成影响，不推荐使用Swap。

          [root@master]# echo "vm.swappiness = 0">> /etc/sysctl.conf 
          [root@master]# swapoff -a        //临时禁用
          
          or
          
          //若需要重启后也生效，在禁用swap后还需修改配置文件/etc/fstab，注释swap
          [root@master]# sed -i.bak '/swap/s/^/#/' /etc/fstab       //永久禁用

## master2虚拟机安装准备

## master3虚拟机安装准备

## node1虚拟机安装准备

## node2虚拟机安装准备

## node3虚拟机安装准备

---

# 2 安装kubeadm等工具

**准备工作完毕后，接着在master、master2,master3,node1、node2、node3四台虚拟机上安装kubeadm相关工具。下面这些操作需要在master、master2,master3,node1、node2、node3四台虚拟机上执行，这里以master为例**

## master机上安装kubeadm等工具

         1.配置kubernetes源：

          [root@master]# vi /etc/yum.repos.d/kubernetes.repo

                         [kubernetes]
                         name=Kubernetes
                         baseurl= https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/   //配置国内的kubernetes源 
                         or 
                         
                         baseurl= https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64/                  // 配置国外的kubernetes
                         enabled=1
                         gpgcheck=1
                         repo_gpgcheck=1
                         gpgkey= https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg    //国内
                         
                         or 
                         
                         gpgkey= https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg   //国外
                         
                         exclude=kubelet kubeadm kubectl
                         
 ---
                         
                         
                       [] 中括号中的是repository id，唯一，用来标识不同仓库
                       name 仓库名称，自定义
                       baseurl 仓库地址
                       enable 是否启用该仓库，默认为1表示启用
                       gpgcheck 是否验证从该仓库获得程序包的合法性，1为验证
                       repo_gpgcheck 是否验证元数据的合法性 元数据就是程序包列表，1为验证
                       gpgkey=URL 数字签名的公钥文件所在位置，如果gpgcheck值为1，此处就需要指定gpgkey文件的位置，如果gpgcheck值为0就不需要此项了  
         
         2. 更新缓存
            [root@master]# yum clean all
            [root@master]# yum -y makecache
          
         3.安装kubelet、kubeadm和kubectl工具：

           //版本查看
           [root@master]#   yum list kubelet --showduplicates | sort -r 

           //若不指定版本直接运行‘yum install -y kubelet kubeadm kubectl’则默认安装最新版
           // 现在是指定kubernetes 的版本
           
           [root@master]# yum install -y kubelet-1.19.3-0 kubeadm-1.19.3-0 kubectl-1.19.3-0 --disableexcludes=kubernetes

         4.启动kubelet并设置开机自启：     
         
           [root@master]# systemctl enable kubelet
           [root@master]# systemctl start kubelet

           提示：此时kubelet的服务运行状态是异常的，因为缺少主配置文件kubelet.conf。但可以暂不处理，因为在完成Master节点的初始化后才会生成这个配置文件。

## master2机上安装kubeadm等工具

## master3机上安装kubeadm等工具

## node1机上安装kubeadm等工具

## node2机上安装kubeadm等工具

## node3机上安装kubeadm等工具

---

# 3 安装Master节点

## 安装Master节点在master虚拟机上

**安装好kubeadm等相关工具后，接着在master虚拟机上执行以下操作：使用下面这条命令启动 master节点**

    3.1 初始化Master
        
        //使用 kubeadm 初始化集群（只在master节点上执行）
        [root@master]# kubeadm init --kubernetes-version=v1.19.3    
                                    --pod-network-cidr=10.244.0.0/16 
                                    --service-cidr=10.1.0.0/16 
                                    --apiserver-advertise-address=192.168.33.11 
                                    //--image-repository registry.aliyuncs.com/google_containers
      
      配置含义如下：

       kubernetes-version: 用于指定k8s版本，这里指定为最新的1.19.3版本；
       apiserver-advertise-address：用于指定kube-apiserver监听的ip地址，就是master本机IP地址。
       pod-network-cidr：因为后面我们选择flannel作为Pod的网络插件，所以这里需要指定Pod的网络范围为10.244.0.0/16
       service-cidr：用于指定SVC的网络范围；
       image-repository: 其中默认的镜像仓库k8s.gcr.io没有科学上网的话无法访问，我们可以将它修改为国内的阿里镜像仓
                         库registry.aliyuncs.com/google_containers

       启动成功后，你会看到类似如下提示:
       
            Your Kubernetes control-plane has initialized successfully!

            To start using your cluster, you need to run the following as a regular user:

              mkdir -p $HOME/.kube
              sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
              sudo chown $(id -u):$(id -g) $HOME/.kube/config

            You should now deploy a pod network to the cluster.
            Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
              https://kubernetes.io/docs/concepts/cluster-administration/addons/

            Then you can join any number of worker nodes by running the following on each as root:

            //而下面这段则是用于工作节点Node加入Master集群用的，而它的令牌号数是每次执行 kubernetes init 命令都不同的
            
            kubeadm join 192.168.33.11:6443 --token 0na0jo.e9svsj5tmeciad88 --discovery-token-ca-cert-hash sha256:5fbcc1a1167d0
                                      db98d0fc6c037a06378899978d4c811c61a271f2bf116810e93
            
   **该 token 的有效时间为 2 个小时，2小时内，您可以使用此 token 初始化任意数量的 worker 节点。**
   
           如果在这两个小时内要重新找回该token，就只能在master虚拟机上输入如下命令：
   
           # 只在 master 节点执行
           [root@master ~]# kubeadm token create --print-join-command
           
            kubeadm join 192.168.33.11:6443 --token 0na0jo.e9svsj5tmeciad88 --discovery-token-ca-cert-hash sha256:5fbcc1a1167d0
                                      db98d0fc6c037a06378899978d4c811c61a271f2bf116810e93
       
            
            
  <a href="https://ibb.co/wMsPnZ7"><img src="https://i.ibb.co/6gBTKc8/kubeadm-token-number.jpg" alt="kubeadm-token-number" border="0"></a>
            
          等待一段时间，看到successful即说明Kubernetes集群初始化完成。加载kubeadm认证到系统环境变量中:
            
          //加载kubeadm认证到系统环境变量           
          // 注意：想要操作k8s的用户，必须在当前用户环境下生成 ~/.kube文件
          
         //如果是root用户
         [root@master ~]# echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /vagrant/.bash_profile
         [root@master ~]# cd /vagrant
         [root@master vagrant]# source .bash_profile 

         or 
         
         //若为非root用户
         [root@master]# sudo mkdir -p $HOME/.kube
         [root@master]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
         [root@master]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
       
       初始化失败：

         如果初始化失败，可执行kubeadm reset后重新初始化
         [root@master01 ~]# kubeadm reset
         [root@master01 ~]# rm -rf $HOME/.kube/config
       
       
  
        
        
# 4 安装Node节点

**接着在node1、node2、node3三台虚拟机上安装Node节点，在node1、node2和node3节点上执行下面这条命令，加入Master**
**注意， 在Node 节点运行kubeadm join 命令时需要使用Root 用户（或使用sudo 执行）, 因为kube-proxy 服务需要操作系统的iptables 服务，必须获得管理员权限**

## 在node1虚拟机上安装Node节点
   
   // 在工作节点中清除之前的环境，必需要执行这步，不然会出错
   [root@node1]# kubeadm reset

   [root@node1]# rm -rf $HOME/.kube/config

   //关闭缓冲区
   [root@node1]# swapoff -a 
   
   [root@node1]# systemctl enable --now kubelet
   
   [root@node1]# systemctl start kubelet
   
   [root@node1]# kubeadm join 192.168.33.11:6443 --token 0na0jo.e9svsj5tmeciad88 --discovery-token-ca-cert-hash sha256:5fbcc1a1167d0
                                      db98d0fc6c037a06378899978d4c811c61a271f2bf116810e93

    为了确定该Node1节点是否加入了集群，可在master虚拟机上输入
  
    [root@master]# kubectl get nodes
  
    可看到该Node1 节点

<a href="https://ibb.co/Cs4DzRh"><img src="https://i.ibb.co/BKvFt0q/flannel2.png" alt="flannel2" border="0"></a>
  
##  worker 节点不能加入集群的常见错误原因 
     
   1 如果该worker 节点不能加入集群，错误情况如下：
   
     在worker节点执行以下语句可验证worker节点是否能访问 apiserver

     [root@node1]# curl -ik https://192.168.33.11:6443
    
     如果不能，请在 master 节点上验证

     [root@node1]# curl -ik https://localhost:6443
    
     正常输出结果如下所示：

          HTTP/1.1 403 Forbidden
          Cache-Control: no-cache, private
          Content-Type: application/json
          X-Content-Type-Options: nosniff
          Date: Fri, 15 Nov 2019 04:34:40 GMT
          Content-Length: 233

          {
            "kind": "Status",
            "apiVersion": "v1",
            "metadata": {
          ...

   2    如果 master 节点能够访问 apiserver、而 worker 节点不能，则请检查自己的网络设置
   
            /etc/hosts 是否正确设置？
            是否有安全组或防火墙的限制？
   
   3    移除worker节点并重试
   
         正常情况下，您无需移除 worker 节点，如果添加到集群出错，您可以移除 worker 节点，再重新尝试添加
         
         在准备移除的 worker 节点上执行

         # 只在 worker 节点执行
         kubeadm reset -f


         在 master 节点 demo-master-a-1 上执行

         # 只在 master 节点执行
         kubectl get nodes -o wide
    
        如果列表中没有您要移除的节点，则忽略下一个步骤

        # 只在 master 节点执行
        kubectl delete node demo-worker-x-x
   


## 在node2虚拟机上安装Node节点

   // 在工作节点中清除之前的环境，必需要执行这步，不然会出错
   [root@node2]# kubeadm reset

   [root@node2]# rm -rf $HOME/.kube/config

   //关闭缓冲区

   [root@node2]# swapoff -a 
   
   [root@node2]# systemctl enable --now kubelet
   
   [root@node2]# systemctl start kubelet
   
   [root@node2]# kubeadm join 192.168.33.11:6443 --token 0na0jo.e9svsj5tmeciad88 --discovery-token-ca-cert-hash sha256:5fbcc1a1167d0
                                      db98d0fc6c037a06378899978d4c811c61a271f2bf116810e93



## 在node3虚拟机上安装Node节点


   // 在工作节点中清除之前的环境，必需要执行这步，不然会出错
   [root@node3]# kubeadm reset

   [root@node3]# rm -rf $HOME/.kube/config

   //关闭缓冲区
                                      
   [root@node3]# swapoff -a 
   
   [root@node3]# systemctl enable --now kubelet
   
   
   [root@node3]# systemctl start kubelet
   
   [root@node3]# kubeadm join 192.168.33.11:6443 --token 0na0jo.e9svsj5tmeciad88 --discovery-token-ca-cert-hash sha256:5fbcc1a1167d0
                                      db98d0fc6c037a06378899978d4c811c61a271f2bf116810e93

# 5 配置网络
    
    等待上述Node节点的命令执行完毕后，在master节点上执行下面的命令，子节点的名字已经出现在屏幕中了
    在master上执行kubectl get nodes命令，会发现Kubernetes提示Master为NotReady状态，这是因为还没有安装网络插件
    
    [root@master]# kubectl get nodes
        
  <a href="https://ibb.co/Cs4DzRh"><img src="https://i.ibb.co/BKvFt0q/flannel2.png" alt="flannel2" border="0"></a>

    
##  配置 flannel 网络

     //对于网络插件，可以有许多选择，请参考https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network的说明。这里我选择的flannel
     //注意： 为了使Flannel正常工作，执行kubeadm init命令时需要增加--pod-network-cidr=10.244.0.0/16参数。Flannel适用于amd64，arm，arm64和ppc64le上工作，但使用除amd64平台得其他平台，
     //你必须手动下载并替换amd64。
     
        [root@master]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  //一般会出问题 可先下载，
      
        ////在master虚拟机上下载flannel配置文件到当前目录下：
        [root@master]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 
       
        //修改kube-flannel.yml：
        [root@master]# vi kube-flannel.yml
        
        containers:
             - name: kube-flannel
               image: quay.io/coreos/flannel:v0.11.0-amd64
               command:
               - /opt/bin/flanneld
               args:
               - --ip-masq
               - --kube-subnet-mgr
               - --iface=eth1 # 新增部分

<a href="https://ibb.co/5v50VyM"><img src="https://i.ibb.co/vmJF8Gw/eth1.png" alt="eth1" border="0"></a>

         //Vagrant 在多主机模式下有多个网卡，eth0 网卡用于nat转发访问公网，而eth1网卡才是主机真正的IP，在这种情况下直接部署k8s flannel 插件会导致CoreDNS无法工作，所以我们需要添加上面这条
           配置强制flannel使用eth1
           
        //安装flannel：

        [root@master]# kubectl create -f kube-flannel.yml  
        
        输出如下所示时，表示安装成功
        
  <a href="https://ibb.co/WgXpnw7"><img src="https://i.ibb.co/f4WGM5Z/flannel.png" alt="flannel" border="0"></a>
               
        再次查看节点状态：

        [root@master]# kubectl get nodes       
        
  <a href="https://ibb.co/BG3Fd6g"><img src="https://i.ibb.co/hc9whdK/flannel3.png" alt="flannel3" border="0"></a>
        
        
        可以看到所有节点都是Ready状态。
        
        执行kubectl get pods --all-namespaces，验证Kubernetes集群的相关Pod是否都正常创建并运行
        
  <a href="https://ibb.co/bR35rGH"><img src="https://i.ibb.co/5x9h5JY/flannel4.png" alt="flannel4" border="0"></a>        
  
  到这里通过Kubeadm安装Kubernetesv1.19.3 集群已经成功了
  

     
     
     
     
##  配置 Calico 网络


     //对于网络插件，可以有许多选择，请参考https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network的说明。这里我选择的Calico
     
     
# 6 重启Kubernetes集群     

* [重启Kubernetes集群](https://kuboard.cn/install/k8s-restart.html#worker%E8%8A%82%E7%82%B9%E4%B8%8D%E8%83%BD%E5%90%AF%E5%8A%A8)  
* [解决重启搭建好k8s集群的虚拟机后遇到的问题](https://blog.csdn.net/Kobe_1314/article/details/102154303)
* [Kubernetes集群如何重启](https://blog.csdn.net/u013288190/article/details/109450721?utm_medium=distribute.pc_relevant.none-task-blog-baidulandingword-3&spm=1001.2101.3001.4242)
* [如何重新引导K8s集群？](https://blog.csdn.net/lzy_zhi_yuan/article/details/107031958?utm_medium=distribute.pc_relevant.none-task-blog-baidulandingword-3&spm=1001.2101.3001.4242)
